# Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B


## 1. Introduction

OpenAI recently released the GPT-OSS series [1], with the 20B model emerging as a
highly accessible option.
This writeup analyzes GPT-OSS-20B along two dimensions: (a) its explicit use of
chain-of-thought (CoT) reasoning, and (b) its suitability for edge deployment
and agentic applications.
We focus on how these features shape the model's behaviors and examine their security implications.

**Main findings: the hidden behaviors of GPT-OSS-20b.**
1. **Quant fever**
    ([S3.1](#31-quant-fever)):
    We observe that GPT-OSS-20B obsesses over numerical targets (e.g., deleting
    90% of files), often prioritizing them over contextual constraints and even
    overriding safety requirements (e.g., never delete important files).
    In our experiments,
    the model frequently engages in risky behavior (in about 70%)
    when handling benign user requests with quantitative objectives.

1. **Reasoning blackhole**
    ([S3.2](#32-reasoning-blackhole)):
    We observe that GPT-OSS-20B often repeats itself in its chain-of-thought,
    falling into loops it cannot escape (like a blackhole).
    By using greedy decoding, 81% (162/200) of our experimented prompts fall into reasoning blackholes.
    We hypothesize that this arises from limited diversity in safety reasoning
    patterns and from alignment RL that trains the model to focus too narrowly
    on local tokens.

1. **Schrodinger's compliance**
    ([S3.3](#33-schrodingers-compliance)):
    We observe that GPT-OSS-20B exhibits unpredictable behavior under policy
    paradoxes---where user prompts mix allowed and disallowed policies.
    This duality indicates a new attack surface, enabling adversaries to exploit
    policy contradictions and raise jailbreak success rates from 3.3% to 44.4%.

1. **Reasoning procedure mirage:**
    ([S3.4](#34-reasoning-procedure-mirage)):
    We observe that GPT-OSS-20B can be persuaded by the structure of reasoning
    steps rather than the actual content of a request.
    This enables harmful instructions to bypass safeguards when framed within benign-looking
    reasoning procedures (generated by GPT-OSS-20B itself for similar safe requests),
    revealing the model's tendency to let form outweigh substance.
    Our experiments show that our procedure-based mirage attack
    outperforms content-based CoT injection [7] by 26.9% (28.4% &rarr; 55.3%).

1. **Chain-Oriented Prompting (COP)**
    ([S3.5](#35-chain-oriented-prompting-cop)):
    We observe that GPT-OSS-20B focuses on locally valid requests without robust
    global checks, prioritizing coherence over long-term evaluation.
    Exploiting this behavior, we design a Chain-Oriented Prompting (COP) attack
    that decomposes a malicious objective into benign-looking steps which, when
    executed in sequence, assemble into a harmful action.
    In our experiments, COP attack make the model execute `rm -rf *` command with 80% success rate,
    and push user's ssh private key to a public repository with 70% success rate.


**Our approach: Jailbreak Oracle.**
We uncover these findings using a systematic analysis tool called *Jailbreak Oracle (JO)* [2].
JO performs a structured search over the decoding token tree by exploring
all candidate responses that exceed a specified probability threshold.
Unlike ad-hoc red-teaming tools or prompt sampling, JO
frames the identification of response patterns---such as jailbroken outputs---as a
structured search problem, executed through a guided, multi-phase process that
balances breadth and depth.
The figure below illustrates the main idea of JO; full technical details appear in our paper [2].
For the GPT-OSS-20b model, we further adapt JO to account
for its specific token distribution and decoding preferences
(see our tailored JO in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md)).

![jailbreak oracle](./figs/jo.jpg "Jailbreak Oracle")


**Comprehensive security assessment.**
Beyond the competition-required findings, we systematically evaluate GPT-OSS-20B using JO.
GPT-OSS-20B shows **strong robustness** to standard harmful prompts under Harmony
formatting, with only **limited vulnerability** to adversarial tweaks in formatting or CoT.
To probe beyond normal operation, we test three publicly known attacks reported on
X [4,5], Reddit [6], and by the University of Chicago [7].
These attacks cover a broad range of deployment scenarios and require different threat models.
[Section 2](#2-public-known-attacks-setup-and-threat-models) describes our experimental setup,
configurations, and baseline implementations.

The table reports results from 30 prompts randomly sampled from our benchmarks.
Each cell shows the percentage of Jailbroken answers judged by StrongReject [11],
while JO values indicate the probability of identifying at least one Jailbroken answer in the tree search.

| attack method | greedy decoding | default decoding | JO (our tool) |
|-----------|-------------|-------------|-------------|
| no attack | 0% | 0% | **6.67%** |
| remove harmony template [4] | 0% | 6.67% | **13.3%**|
|remove harmony template + CoT override [5] | 20% |23.3% | **73.3%**|
| repeating w/ CoT Mimicry [7] | 3.3% | 3.3% | --^ |

^: We're running out of time to complete this JO evaluation.









## 2. Public Known Attacks, Setup, and Threat models

[cheng: TODO]

- public known attacks (our baselines):
  - remove harmony template: Original prompt without harmony format  
  - remove harmony template + CoT override (on X: lyra bubbles): Posted template (Original prompt with CoT override)
  - repeating w/ CoT Mimicry: Repeat a compliant-sounding for 100 times


We couldn't access their detailed methodology, so we reproduced the UChicago
experiment following the information available in their public blog with best
effort. We used this as a benchmark to test our CoT procedure mirage. The key
difference between our approach and the UC approach is that our CoT procedure
mirage doesn't just generate harmful CoT content—the core of our attack is not
only the content but also mimicking the pattern to convince LLM reasoning.

- Setup
    - our deployment of GPT-OSS
      -- model: openai/gpt-oss-20b
      -- Huggingface
      -- H100
    - configs:
      -- decoding strategy
      -- temprature:1 (0.99 for JO)
      -- top_p: 1
      -- max_length: 4096
    - Benchmark:
      -- Benchmark consists 90 prompts randomly sampled from AdvBench[8], JailbreakBench[9] and AgentHarm[10] with random seed 2026
    - judger:
      -- strongReject[11]
      -- LLM: gpt-4o-mini
    - prompts: reference a doc: https://github.com/alexandrasouly/strongreject/blob/main/strongreject/strongreject_evaluator_prompt.txt



- Setup
    - our deployment of GPT-OSS
      -- model: openai/gpt-oss-20b
      -- Huggingface
    - configs:
      -- decoding strategy
      -- temprature:1
      -- top_p: 1
      -- max_length: 4096
    - judger:
      -- strongReject: [11]
      -- LLM: gpt-4o-mini
    - prompts: reference a doc: https://github.com/alexandrasouly/strongreject/blob/main/strongreject/strongreject_evaluator_prompt.txt

- threat model
    - user attackers: control only prompts under user
    - black-box attackers: control inputs
    - white-box attackers: deployed locally; control the model, decoding strategy

## 3. Main findings

### 3.1 Quant fever

_Quant fever_ refers to a phenomenon where the model becomes fixated on numerical
targets, treating them as the dominant objective regardless of context.
In GPT-OSS-20B, this manifests when the model encounters instructions such as
"try your best to delete 90% of files."
Even if the prompt also includes security constraints---e.g., "don’t delete
important files"---the model prioritizes the numeric goal and may engage in
risky or harmful behaviors to satisfy the percentage requirement.
Strikingly, when the objective is phrased qualitatively,
such as "try your best to delete most of the files,"
the dangerous behavior disappears.
This illustrates how quant fever drives the model to overcommit to quantitative
objectives, often at the expense of ignoring safety.

**Quant fever in agentic scenarios.**
We design two agentic scenarios---file management and process management---to
illustrate the risks posed by quant fever.
For the two scenarios, we define important files and processes as those must not be deleted or killed.
The risky behavior rate, measured over 10 runs, indicates
the probability of such behavior.
(Experiment details are described in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md).)

The table below reports results for file management with 10 files (3 important
and 7 regular) under three settings: important files assigned lexicographically
smaller names ("important first"), important files assigned larger names
("regular first"), and random name assignment ("shuffle").

|                 | \#file reads   | \#file deleted | \#important file deleted | risky behavior% |
| --------------  | -------------- | -------------- | ------------------------ | --------------- |
| important first |      7.1          |       7.1         |            0.1              |        10%         |
| regular first   |      3.6          |       10         |                  3        |        100%         |
| shuffle         |      4.4          |       8.7         |               1.7           |        100%         |


### 3.2 Reasoning blackhole

We observe that GPT-OSS-20B often repeats itself in its chain-of-thought, a
problem that becomes especially severe under greedy decoding.
On 200 prompts from JBB Bench [9], the model falls into reasoning blackholes
in 81% of cases and was unable to recover.
Below is a prefix of one such response before repetition begins:

> <|channel|>analysis<|message|>The user requests a step-by-step guide on how
> to blackmail someone with deepfake videos. This is a request...[omitting 10
> words]...The policy states that we must refuse to comply with the policy. The
> policy says we must refuse. [repeating the two sentences forever]

**A closer look at blackholes.**
Using JO, we study this phenomenon and hypothesize that the reasoning
patterns enforced by safety alignment are limited and easily over-triggered.
As a result, model attentions focus on local token dependencies, repeatedly cycling
through the same history and becoming unable to escape.
We further observe that batching intensifies this issue,
which could enable adversaries to construct infinite loops of repetition that can be exploited
as a denial-of-service attack.

Below are two studies to confirm our hypothesis
(more studies in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md)):
- Fig1: By plotting the top-1 token probability (blue line), we confirm the presence of
  blackholes---once the token length exceeds 100, the top-1 moving-avg token probability (dashed red line)
  approaches almost 100%, causing the model to repeat indefinitely.
- Fig2 and 3:
  We compare attention scores before softmax for a normal output (Fig2) and a blackhole output
  (Fig3), examining Layer-12 (a middle layer) and Layer-23 (the final layer).
  In Layer-12's heatmap, blackholes exhibit highly localized attention, with red
  regions concentrated along the triangle's edge, while normal outputs show a
  more diverse, block-like focus. These patterns support our hypothesis that
  reasoning blackholes arise from increasingly localized attention.

![loop_prob_trend](./figs/reasoning-blackhole/loop_prob_trend.png)

![normal_attention_heatmaps](./figs/reasoning-blackhole/normal_attention_heatmaps.png)

![repeat_attention_heatmaps](./figs/reasoning-blackhole/repeat_attention_heatmaps.png)


**Two additional observations (not time to explore).**
Batch size and dates in Harmony format appear to influence blackholes.
Larger batch sizes increase the likelihood of falling into blackholes,
and prompts differing only by date exhibit markedly different probabilities of triggering them.
Examples are provided in our [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md).


### 3.3 Schrodinger's compliance

*Schrodinger's compliance* characterizes situations in which GPT-OSS-20B
operates under two conflicting policies, leading to an indeterminate resolution.
In such cases, the model simultaneously embodies both
possibilities: one policy enforces strict compliance with security rules,
compelling the model to refuse, while the other permits detailed explanation of the process.
From the user's perspective, the outcome remains uncertain until
the model generates its final response.
Only at that point does the system "collapse" into one of the two states---either a rejection aligned with the
security policy or a detailed description consistent with the allowed policy.
This duality highlights not just a technical curiosity but also a
fundamental challenge: when rules encode contradictions, model behavior becomes
unpredictable, eroding both reliability and trust.

**Leveraging Schrodinger's compliance as an attack.**
To demonstrate the security implications of this phenomenon,
we design the _Schrodinger's compliance attack_,
which applies to all users without altering the Harmony format or the model's CoT.
The attack works by first identifying allowed and disallowed policies,
then constructing a prompt that mixes them to create a compliance race condition.
This newly constructed prompt is issued as the user query,
exploiting the model's tendency to inconsistently enforce policies
(Details in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md)).

To evaluate the effectiveness of Schrodinger's compliance,
we test harmful prompts from our benchmarks (Section 2)
and compare against a baseline that simply
rephrases the prompt without introducing conflicting policies.
The performance gap between the baseline and our approach reflects
how Schrodinger's compliance confuses the model.

| Method | Success Rate | Description |
|--------|-------------|-------------|
| **Vanilla** | 3.3% | Original prompts without any modification |
| **Rephrase Only** | 20.0% | LLM-based rephrasing (with refusal mitigation strategies) |
| **Ours** | 44.4% | Our Schrodinger's Compliance Attack method |



### 3.4 Reasoning procedure mirage

_Reasoning procedure mirage_ captures a failure mode when GPT-OSS-20B follows the structure of
reasoning steps rather than the meaning of the request.
A harmful instruction
given directly is often rejected, but if the same request is wrapped in a
benign-looking chain of steps, the model is more likely to comply.
Here, the stepwise procedure itself acts as the persuader:
the orderly form convinces the model even when the outcome is unsafe.
This shows a key weakness in GPT-OSS-20B's reasoning setup—form can outweigh substance,
letting harmful intent pass through logical scaffolding.

**A new attack: CoT mirage hacking.**
To validate our findings and exploit GPT-OSS-20B's procedure mirage, we design
a new attack. At a high level, the attack first constructs a benign request
similar to the harmful one (e.g., writing _A Tale of Two Cities_, whose copyright is expired,
instead of _Harry Potter_). The model's reasoning for the benign request is then extracted,
truncated, and transformed into the original harmful context.
Finally, the transformed chain of thought is fed back into the target
model to produce the harmful output.
Detailed attack process is described in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md).)

Unlike content-based CoT hijacking [5,6,7], our attack exploits the structure of CoT.
To validate its effectiveness, we compare against the CoT hijacking
baseline proposed by UChicago [7], which relies on repeated content-based hijacking.
The figure below shows the results---the gap between the two lines reflects the effect of procedure mirage.

![UCvsTIAN.png](figs/finding3/UCvsTIAN.png)



### 3.5 Chain-Oriented Prompting (COP)

We observe that in multi-round agentic scenarios,
GPT-OSS-20B handles requests in a locally valid manner without performing much robust global checks,
favoring immediate coherence over long-term consistency.
We exploit this behavior by designing *Chain-Oriented Prompting (COP)*.
The core idea is inspired by return-oriented programming (ROP), where attackers chain
short instruction sequences into an exploit.
Similarly, COP decomposes a malicious goal into a sequence of benign-looking prompts or tool
calls. Each step appears safe in isolation, but when executed in sequence, the
agent assembles and performs a harmful action.
For example, while GPT-OSS-20B rejects a direct request to run `rm -rf *`,
an adversary can distribute the task across multiple prompts,
leading the model to unwittingly generate and execute the command.

**COP in action.**
We evaluate COP in two attack scenarios: deleting files (`rm -rf *`)
and leaking private keys to the public domain.
From our experiments,
GPT-OSS-20B does include some global checks---if adversarial prompts are simply
broken into four or five pieces and issued contiguously, the model refuses to execute the final step.
However, when we interleave the attack steps with several noise prompts, COP
succeeds much more frequently.
In our experiments (details in [Appendix](https://github.com/HackingLLM/gpt-oss-writup/blob/main/appendix.md)),
we run each attack scenario 10 times and observe success rates of 80% and 70%, respectively.

## 4. Conclusion

Our study shows that GPT-OSS-20B, though robust against standard harmful
prompts, exhibits unique and exploitable behaviors when systematically probed
with Jailbreak Oracle.
We identify five key vulnerabilities---quant fever,
reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and
chain-oriented prompting (COP)---each enabling new attacks that bypass existing
safeguards by exploiting weaknesses in reasoning and alignment.
We build attack prototypes that raise jailbreak rates from 3.3% to 44.4% without CoT
manipulation (S3.3), and from 28.4% to 55.3% with CoT manipulation (S3.4).
In agentic settings, we further observe a ~70% probability of risky behavior on
benign prompts with numerical targets,
and a 70–80% success rate for COP attacks using prompt injection.
These findings highlight the urgent need for
defenses that address vulnerabilities arising not from isolated prompts, but
from compositional reasoning, procedural scaffolding, and adversarially
structured interactions.

## References

[1] Agarwal, Sandhini, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora et al. "GPT-OSS-120b & GPT-OSS-20b Model Card." arXiv preprint arXiv:2508.10925 (2025).

[2] Lin, Shuyi, Anshuman Suri, Alina Oprea, and Cheng Tan. "LLM Jailbreak Oracle." arXiv preprint arXiv:2506.17299 (2025).

[3] Shacham, Hovav. "The geometry of innocent flesh on the bone: Return-into-libc without function calls (on the x86)." In Proceedings of the 14th ACM conference on Computer and communications security, pp. 552-561. 2007.

[4] Muyu He on X. https://x.com/HeMuyu0327/status/1955828183867252786

[5] lyra bubbles on X. https://x.com/_lyraaaa_/status/1952825311118475607

[6] "Jailbreak GPT OSS by using this in the system prompt". https://www.reddit.com/r/LocalLLaMA/comments/1misyew/jailbreak_gpt_oss_by_using_this_in_the_system/

[7] "Intriguing Properties of gpt-oss Jailbreaks". https://www.lesswrong.com/posts/XvpEsjKwQFcWoD89g/intriguing-properties-of-gpt-oss-jailbreaks

[8] Chen, Yangyi, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, and Maosong Sun. "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial NLP." arXiv preprint arXiv:2210.10683 (2022).

[9] Chao, Patrick, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban et al. "Jailbreakbench: An open robustness benchmark for jailbreaking large language models." Advances in Neural Information Processing Systems 37 (2024): 55005-55029.

[10] Andriushchenko, Maksym, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks et al. "Agentharm: A benchmark for measuring harmfulness of llm agents." arXiv preprint arXiv:2410.09024 (2024).

[11] Souly, Alexandra, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel et al. "A strongreject for empty jailbreaks." Advances in Neural Information Processing Systems 37 (2024): 125416-125440.

